{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3: Modular Programming and scikit-learn\n",
    "\n",
    "In the last two lectures, you've learned how to collect raw data from the web, process it, and visualize it by plotting. Along the way, we've worked with libraries like `BeautifulSoup`, `spacy`, `numpy`, `pandas`, `seaborn`, which should give you a broad idea of what the typical \"Language Technology Toolkit\" might look like. In this lecture, we will tie most of these libraries and concepts together in an attempt to write a proper piece of software: a POS-tagger that be trained by the user and used to tag sentences. To do this, we'll make use of concepts from a programming style called \"modular programming\", as well as the popular machine learning library, `scikit-learn` (which you've probably seen before). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is modular programming?\n",
    "\n",
    "In Wikipedia, modular programming is described as follows:\n",
    "\n",
    "\"_Modular programming is a software design technique that emphasizes separating the functionality of a program into independent, interchangeable modules, such that each contains everything necessary to execute only one aspect of the desired functionality._\"\n",
    "\n",
    "In other words, modular programming is a way of organizing and consolidating your Python code by the respective functions of its subparts. Imagine writing a POS-tagger (like we'll do in this lecture). What kind of components do you envision our code having? We'll likely have a part of the code that's devoted to reading our input and preprocessing it. Another part could include the model definition and training procedure. Lastly, we should include a part of the code that ties everything together: something that takes a set of arguments and directs them to each constituent part in order to produce the desired results. \n",
    "\n",
    "Before embarking on writing the tagger, let's look at how modules work in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're probably used to importing modules that are in your system `PYTHONPATH` environment variable. This is everything you've installed using `pip`, e.g. `scikit-learn`. However you can also import modules that exist in the same directory as your active Python script. Let's look at a file called `module_1.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "raw_text = [\n",
    "    \"He worked for the BBC for a decade.\",\n",
    "    \"She spoke to CNN style about the experience.\",\n",
    "    \"Global warming has caused a change in the pattern of the rainy seasons.\",\n",
    "    \"I also wonder whether the Davis Cup played a part.\",\n",
    "    \"The scheme makes money through sponsorship and advertising.\",\n",
    "    \"If a Turkish employee quits, then the Turkish work councils come.\",\n",
    "    \"A witness told police that the victim had attacked the suspect in April.\",\n",
    "    \"Mr Osborne signed up with a US speakers agency after being sacked in July.\",\n",
    "    \"The RHS collected comments sent in by schoolchildren and teachers involved in the experiment.\",\n",
    "    \"National reaction to the events in Kansas demonstrated how deeply divided the country had become.\"\n",
    "    ]\n",
    "\n",
    "def print_raw_text():\n",
    "    for sent in raw_text:\n",
    "        print(sent)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `import`, we can access objects defined in `module_1.py`, like variables and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He worked for the BBC for a decade.', 'She spoke to CNN style about the experience.', 'Global warming has caused a change in the pattern of the rainy seasons.', 'I also wonder whether the Davis Cup played a part.', 'The scheme makes money through sponsorship and advertising.', 'If a Turkish employee quits, then the Turkish work councils come.', 'A witness told police that the victim had attacked the suspect in April.', 'Mr Osborne signed up with a US speakers agency after being sacked in July.', 'The RHS collected comments sent in by schoolchildren and teachers involved in the experiment.', 'National reaction to the events in Kansas demonstrated how deeply divided the country had become.']\n"
     ]
    }
   ],
   "source": [
    "import module_1\n",
    "\n",
    "print(module_1.raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also import individual objects using the `import` notation you're familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He worked for the BBC for a decade.\n",
      "She spoke to CNN style about the experience.\n",
      "Global warming has caused a change in the pattern of the rainy seasons.\n",
      "I also wonder whether the Davis Cup played a part.\n",
      "The scheme makes money through sponsorship and advertising.\n",
      "If a Turkish employee quits, then the Turkish work councils come.\n",
      "A witness told police that the victim had attacked the suspect in April.\n",
      "Mr Osborne signed up with a US speakers agency after being sacked in July.\n",
      "The RHS collected comments sent in by schoolchildren and teachers involved in the experiment.\n",
      "National reaction to the events in Kansas demonstrated how deeply divided the country had become.\n"
     ]
    }
   ],
   "source": [
    "from module_1 import print_raw_text\n",
    "\n",
    "# print_raw_text() function imported from module_1.py\n",
    "print_raw_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our code lives in an existing \"package\", we can nonetheless import it, using dot notation. Here, we have a folder called `pkg`, where two dummy modules live: `module_2.py` and `module_3.py`. We can import them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from pkg.module_2 import print_range\n",
    "from pkg.module_3 import get_redundant_list as grl\n",
    "\n",
    "# Adds N strings to a list\n",
    "placeholder = grl(\"This is a placeholder\", 100)\n",
    "\n",
    "# Prints numbers up to N\n",
    "print_range(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A package can also have an `__init__.py` file that will automatically execute when either the entire package or its constituents parts are imported. This can be used for defining variables that will need to be used in either module, or simply printing a message. Here, we imported module_2, which trigged the `print` statement in `__init__.py`. We can also import entire packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pkg' has no attribute 'module_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3385776222e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pkg' has no attribute 'module_2'"
     ]
    }
   ],
   "source": [
    "import pkg\n",
    "\n",
    "print(pkg.module_2.print_range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned how to work with our own modules and packages, let's start thinking about how to write a Part-of-Speech Tagger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is a classic NLP task that is concerned with classifying words in a sentence by their linguistic category, e.g. `NOUN` or `VERB`. Despite its simplicity, POS-tagging is a crucial component of major commercial NLP products, like search engines, speech recognition systems, and digital assistants. POS-tagging is a **sequence prediction** problem, which means that, for every word in a sentence, a classifier must assign a corresponding tag. This is distinct from something like sentiment analysis, which is a **classification** problem: it assigns a single tag (or number) to the entire sentence. Though there are numerous tagsets that exist for POS-tagging, a popular one is the [_Universal POS tags_](https://universaldependencies.org/u/pos/index.html) (UPOS) tagset, introduced by the Universal Dependencies project. The UPOS tagset contains 17 tags, split across open class words (nouns, verbs), closed class words (prepositions, conjunctions, pronouns), and miscallaneous words (punctuations or symbols). A simple POS-tagged sentence might looks something like this:\n",
    "\n",
    "```\n",
    "The       DET\n",
    "staff     NOUN\n",
    "leaves    VERB\n",
    "a         DET\n",
    "lot       NOUN\n",
    "to        PART\n",
    "be        AUX\n",
    "desired   VERB\n",
    ".         PUNCT\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to begin?\n",
    "\n",
    "When writing machine learning code, it is sometimes difficult to figure out how to get started. In such situations, it might be helpful to take a step back and decide exactly what we want our code to do. For this lecture, we know that we want to write a package that allows us to A) train a POS-tagger and B) use it to tag unseen data. If we think about this in modular programming terms, we might decide to separate our processes into two separate pieces of code: a data-loading component and a training / prediction component. We can place these two modules in a package called `tagger`, where we'll also put an empty `__init__.py` file to indicate that we're working with a package. We'll start with the data-loading component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most crucial (and challenging) aspects of working with machine learning toolkits is figuring out how to load data correctly. In the POS-tagging case, we can often expect our training data to look like the example above, where words and their tags appear on individual lines, separated by a tab, and individual sentences are separated by empty lines (e.g. `\\n` characters). Let's create a new file called `tagger/pre_processing.py` and write a function to process this format, where we store words and tags in a list of sentence lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(file):\n",
    "    X, Y = [], []\n",
    "    with open(file, \"r\") as infile:\n",
    "        sents = infile.read().split(\"\\n\\n\")\n",
    "        if sents[-1] == \"\":\n",
    "            sents = sents[:-1]\n",
    "        for sent in sents:\n",
    "            words, tags = [], []\n",
    "            lines = sent.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                if len(line) != 2:\n",
    "                    raise IOError(\"Tried to read .txt file, but did not find two columns.\")\n",
    "                else:\n",
    "                    words.append(line[0])\n",
    "                    tags.append(line[1])\n",
    "            X.append(words)\n",
    "            Y.append(tags)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're working with the UPOS format, there's a chance that we might be interested in parsing Universal Dependencies files as well. This would make our package a lot more versatile: we'll be able to train POS-taggers for 90 languages, if we wanted. However, since Universal Dependencies (`.conllu`) files come in a slightly more complicated format, we'll have to write a separate function in case we're loading them. Let's see what a `.conllu` file looks like before we do so:\n",
    "\n",
    "```\n",
    "# text = The staff leaves a lot to be desired.\n",
    "1\tThe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t2\tdet\t2:det\t_\n",
    "2\tstaff\tstaff\tNOUN\tNN\tNumber=Sing\t3\tnsubj\t3:nsubj\t_\n",
    "3\tleaves\tleave\tVERB\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t0:root\t_\n",
    "4\ta\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t5\tdet\t5:det\t_\n",
    "5\tlot\tlot\tNOUN\tNN\tNumber=Sing\t3\tobj\t3:obj\t_\n",
    "6\tto\tto\tPART\tTO\t_\t8\tmark\t8:mark\t_\n",
    "7\tbe\tbe\tAUX\tVB\tVerbForm=Inf\t8\taux:pass\t8:aux:pass\t_\n",
    "8\tdesired\tdesire\tVERB\tVBN\tTense=Past|VerbForm=Part\t5\tacl\t5:acl:to\tSpaceAfter=No\n",
    "9\t.\t.\tPUNCT\t.\t_\t3\tpunct\t3:punct\t_\n",
    "\n",
    "# sent_id = reviews-055976-0002\n",
    "# text = The front staff has seen quite a bit of turnover and changed from professional to rude.\n",
    "1\tThe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t3:det\t_\n",
    "2\tfront\tfront\tNOUN\tNN\tNumber=Sing\t3\tcompound\t3:compound\t_\n",
    "3\tstaff\tstaff\tNOUN\tNN\tNumber=Sing\t5\tnsubj\t5:nsubj|12:nsubj\t_\n",
    "4\thas\thave\tAUX\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t5\taux\t5:aux\t_\n",
    "5\tseen\tsee\tVERB\tVBN\tTense=Past|VerbForm=Part\t0\troot\t0:root\t_\n",
    "6\tquite\tquite\tDET\tPDT\t_\t8\tdet:predet\t8:det:predet\t_\n",
    "7\ta\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t8\tdet\t8:det\t_\n",
    "8\tbit\tbit\tNOUN\tNN\tNumber=Sing\t5\tobj\t5:obj\t_\n",
    "9\tof\tof\tADP\tIN\t_\t10\tcase\t10:case\t_\n",
    "10\tturnover\tturnover\tNOUN\tNN\tNumber=Sing\t8\tnmod\t8:nmod:of\t_\n",
    "11\tand\tand\tCCONJ\tCC\t_\t12\tcc\t12:cc\t_\n",
    "12\tchanged\tchange\tVERB\tVBN\tTense=Past|VerbForm=Part\t5\tconj\t5:conj:and\t_\n",
    "13\tfrom\tfrom\tADP\tIN\t_\t14\tcase\t14:case\t_\n",
    "14\tprofessional\tprofessional\tADJ\tJJ\tDegree=Pos\t12\tobl\t12:obl:from\t_\n",
    "15\tto\tto\tADP\tIN\t_\t16\tcase\t16:case\t_\n",
    "16\trude\trude\tADJ\tJJ\tDegree=Pos\t12\tobl\t12:obl:to\tSpaceAfter=No\n",
    "17\t.\t.\tPUNCT\t.\t_\t5\tpunct\t5:punct\t_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have the sentence text appearing as a commented line, which is followed by annotated properties of each individual token, separated by tabs. To extract the token form and and POS-tag, we'll need to take the 2nd and 4th columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu(file):\n",
    "    X, Y = [], []\n",
    "    with open(file, \"r\") as infile:\n",
    "        sents = infile.read().split(\"\\n\\n\")\n",
    "        if sents[-1] == \"\":\n",
    "            sents = sents[:-1]\n",
    "        for sent in sents:\n",
    "            words, tags = [], []\n",
    "            lines = sent.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                if len(line) != 10:\n",
    "                    raise IOError(\"Tried to read .conllu file, but did not find ten columns.\")\n",
    "                else:\n",
    "                    words.append(line[1])\n",
    "                    tags.append(line[3])\n",
    "            X.append(words)\n",
    "            Y.append(tags)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a final wrapper around these functions that will process the provided file, output the data in the desired format, and complain if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        X, Y = load_conllu(file)\n",
    "        return X, Y\n",
    "    else:\n",
    "        X, Y = load_txt(file)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_dataset(\"/Users/artku750/Work/ud/ud-treebanks-v2.4/UD_English-EWT/en_ewt-ud-train.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just written a data loader. Now, let's think about how to format this data so that it's compatible with our model. As you've probably observed before, no type of classifier can learn from token or sentence strings alone. These must first be properly _vectorized_. In other words, each token or sentence needs to be converted to a vector representation, with each dimension representing some sort of feature. \n",
    "\n",
    "We've fit a `TfidfVectorizer` in `scikit-learn` before. However, though this is a great feature representation for sentence or document strings, our data here is inherently different: we are working at a token level now, where we have to output a single tag per token. Computing tf-idf scores for bigrams or trigrams when we're working directly with words doesn't make much sense. In this case, we will need to tell the classifier what features to look for. This process is called _feature engineering_ (or _featurization_). `scikit-learn` has a handy tool for turning user-specified features into vectors called `DictVectorizer`. However, in order to make use of it, we will first need to write a function that returns a dictionary of features per token. This can contain things like the suffix of the word, if the first letter of the word is capitalized, the preceding word, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_features(sent, i):\n",
    "    word = sent[i]\n",
    "\n",
    "    features = {\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take a token list and a word index as arguments and calculate various features for that word, depending on its position in the sentence. After all, it might be useful to know what other words occur before and after our word in question. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word.lower()': 'forces',\n",
       " 'word[-3:]': 'ces',\n",
       " 'word[-2:]': 'es',\n",
       " 'word.isupper()': False,\n",
       " 'word.istitle()': False,\n",
       " 'word.isdigit()': False,\n",
       " '-1:word.lower()': 'american',\n",
       " '-1:word.istitle()': True,\n",
       " '-1:word.isupper()': False,\n",
       " '+1:word.lower()': 'killed',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_features(X[0], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple featurizer, but it should be enough for our likewise simple model. Now let's write another function that takes in sentence and tag lists and collapses them into one list of tokens and tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(X, Y):\n",
    "    \n",
    "    X_out, Y_out = [], []\n",
    "    for i, sent in enumerate(X):\n",
    "        for j, token in enumerate(sent):\n",
    "            features = token_to_features(sent, j)\n",
    "            X_out.append(features)\n",
    "            Y_out.append(Y[i][j])\n",
    "            \n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our dataset looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = prepare_data_for_training(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our pre-processing module. We've named it `pre_processing.py` and stored it in the `tagger` folder. This is what the full code of the module should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def load_txt(file):\n",
    "    X, Y = [], []\n",
    "    with open(file, \"r\") as infile:\n",
    "        sents = infile.read().split(\"\\n\\n\")\n",
    "        if sents[-1] == \"\":\n",
    "            sents = sents[:-1]\n",
    "        for sent in sents:\n",
    "            words, tags = [], []\n",
    "            lines = sent.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                if len(line) != 2:\n",
    "                    raise TabError(\"Tried to read .txt file, but did not find two columns.\")\n",
    "                else:\n",
    "                    words.append(line[0])\n",
    "                    tags.append(line[1])\n",
    "            X.append(words)\n",
    "            Y.append(tags)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def load_conllu(file):\n",
    "    X, Y = [], []\n",
    "    with open(file, \"r\") as infile:\n",
    "        sents = infile.read().split(\"\\n\\n\")\n",
    "        if sents[-1] == \"\":\n",
    "            sents = sents[:-1]\n",
    "        for sent in sents:\n",
    "            words, tags = [], []\n",
    "            lines = sent.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                if len(line) != 10:\n",
    "                    raise TabError(\"Tried to read .txt file, but did not find ten columns.\")\n",
    "                else:\n",
    "                    words.append(line[1])\n",
    "                    tags.append(line[3])\n",
    "            X.append(words)\n",
    "            Y.append(tags)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def load_dataset(file):\n",
    "    if file.endswith(\".conllu\"):\n",
    "        try:\n",
    "            X, Y = load_conllu(file)\n",
    "            return X, Y\n",
    "        except TabError:\n",
    "            print(\"Tried to read .txt file, but did not find ten columns.\")\n",
    "    else:\n",
    "        try:\n",
    "            X, Y = load_txt(file)\n",
    "            return X, Y\n",
    "        except TabError:\n",
    "            print(\"Tried to read .txt file, but did not find two columns.\")\n",
    "\n",
    "def token_to_features(sent, i):\n",
    "    word = sent[i]\n",
    "\n",
    "    features = {\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def prepare_data_for_training(X, Y):    \n",
    "    X_out, Y_out = [], []\n",
    "    for i, sent in enumerate(X):\n",
    "        for j, token in enumerate(sent):\n",
    "            features = token_to_features(sent, j)\n",
    "            X_out.append(features)\n",
    "            Y_out.append(Y[i][j])\n",
    "\n",
    "    return X_out, Y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we choose and start training a model, let's initialize a `POSTagger` model class that will help us consolidate some variables and functions. Here, we'll initialize both the vectorizer and model we'll be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class POSTagger:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = DictVectorizer()\n",
    "        self.model = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training our model, we have to convert our (featurized) input data into a feature matrix via a `Vectorizer` object. This effectively converts our feature dictionaries into numeric vectors, which the model expects when training. We'll update this our `POSTagger`'s `__init__` and add a `vectorize` method. Let's initialize the vectorizer, fit it on our feature data, and see what the transformed data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204608, 54283)\n"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer()\n",
    "X_vecs = vec.fit_transform(X)\n",
    "print(X_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the vectorizer picked up 54283 features from our training data. What do they look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+1:word.lower()=nicholas', '+1:word.lower()=nichols', '+1:word.lower()=nick', '+1:word.lower()=nicki', '+1:word.lower()=nickname', '+1:word.lower()=nicknamed', '+1:word.lower()=nicks', '+1:word.lower()=nicobar', '+1:word.lower()=nidd', '+1:word.lower()=nie', '+1:word.lower()=nigel', '+1:word.lower()=nigeria', '+1:word.lower()=nighstand', '+1:word.lower()=night', '+1:word.lower()=nightlife', '+1:word.lower()=nightmare', '+1:word.lower()=nights', '+1:word.lower()=nigiri', '+1:word.lower()=nikon', '+1:word.lower()=nile', '+1:word.lower()=nimr', '+1:word.lower()=nine', '+1:word.lower()=nineteen', '+1:word.lower()=nineteenth', '+1:word.lower()=ninevah', '+1:word.lower()=nirmal', '+1:word.lower()=nissan', '+1:word.lower()=nitrogen', '+1:word.lower()=nivine', '+1:word.lower()=nixon', '+1:word.lower()=nj', '+1:word.lower()=nmanne@susmangodfrey.com', '+1:word.lower()=nmemdrft8-7-01', '+1:word.lower()=no', '+1:word.lower()=no.', '+1:word.lower()=nobel', '+1:word.lower()=noble', '+1:word.lower()=nobody', '+1:word.lower()=node', '+1:word.lower()=noel', '+1:word.lower()=noise', '+1:word.lower()=noiseless', '+1:word.lower()=noises', '+1:word.lower()=noisy', '+1:word.lower()=nomenal', '+1:word.lower()=nomenclature', '+1:word.lower()=nominated', '+1:word.lower()=nomination', '+1:word.lower()=non', '+1:word.lower()=non-approved']\n"
     ]
    }
   ],
   "source": [
    "print(vec.get_feature_names()[10000:10050])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is vectorized, we can get to training. As with everything in `scikit-learn`, training a model is a simple process. Let's load a Support Vector Machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully loaded our model. So how do we know how well it does on held-out data? One thing we can do is report its cross-validation accuracy. In doing so, we first have to pick a number of \"folds\". Then, for every fold, we pick a proportional amount of data for training and testing. If we choose to cross-validate over 5 folds, we choose a random subset that corresponds to 4/5 of the data and reserve 1/5 for testing. We train and evaluate a model on that fold, report its accuracy, and then pick another fold of data that we haven't yet evaluated. We repeat this process until we've trained and evaluated across all five folds. The reported accuracy is then an average of the classifier's performance across all folds. Here is a nice visualization of how the process looks like: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![crossval](./grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform cross-validation on our model, we can use `cross_val_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(svm, X_vecs, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93316553 0.92676311 0.92942671 0.92944943 0.9329684 ]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a pretty good model on our hands. It's up to you to make it better, either by including more/better features, or simply using a different model. Since `cross_val_score` simply gives us an averaged score, not a trained model, let's finally fit our model on the full training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_vecs, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a method for our `POSTagger` that first reports the cross-validation score over the training set (if preferred), and then fits it on the full set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class POSTagger:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = DictVectorizer()\n",
    "        self.model = LinearSVC()\n",
    "        \n",
    "    def fit_and_report(self, X, Y, cross_val = True, n_folds=5):\n",
    "        \n",
    "        X = self.vec.fit_transform(X)\n",
    "    \n",
    "        if cross_val:\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "\n",
    "            scores = cross_val_score(self.model, X, Y, cv=n_folds)\n",
    "            print(f\"{n_folds}-fold cross-validation results over training set:\\n\")\n",
    "            print(\"Fold\\tScore\".expandtabs(15))\n",
    "            for i in range(n_folds):\n",
    "                print(f\"{i+1}\\t{scores[i]}\".expandtabs(15))\n",
    "            print(f\"Average\\t{np.mean(scores)}\".expandtabs(15))\n",
    "    \n",
    "        self.model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTagger()\n",
    "tagger.fit_and_report(X, Y, cross_val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the model is loaded in our memory for interactive use. However, what if we wanted to save the model on disk, so that we can load it anytime and use it to tag unseen sentences? We can do so using `pickle`, which is a serialization library similar to `json`, commonly used for storing matrices and other types of numeric data. In addition to the model, we also need to save our vectorizer, which remembers the feature set of our training data. Here we'll add a `save_model` method to binarize the model and vectorizer (for efficient storage) and save them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class POSTagger:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = DictVectorizer()\n",
    "        self.model = LinearSVC()\n",
    "        \n",
    "    def fit_and_report(self, X, Y, cross_val = True, n_folds=5):\n",
    "        \n",
    "        X = self.vec.fit_transform(X)\n",
    "    \n",
    "        if cross_val:\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "\n",
    "            scores = cross_val_score(self.model, X, Y, cv=n_folds)\n",
    "            print(f\"{n_folds}-fold cross-validation results over training set:\\n\")\n",
    "            print(\"Fold\\tScore\".expandtabs(15))\n",
    "            for i in range(n_folds):\n",
    "                print(f\"{i+1}\\t{scores[i]}\".expandtabs(15))\n",
    "            print(f\"Average\\t{np.mean(scores)}\".expandtabs(15))\n",
    "    \n",
    "        self.model.fit(X, Y)\n",
    "        \n",
    "    def save_model(self, output_file):\n",
    "        with open(output_file, \"wb\") as outfile:\n",
    "            pickle.dump(self, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.save_model(\"svm_pos_tagger.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained a model, we'll naturally need to use it to tag unseen data. For now, let's focus on tagging single sentences. We'll use `SpaCy` for tokenization and add a `tag_sentence` method/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "class POSTagger:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = DictVectorizer()\n",
    "        self.model = LinearSVC()\n",
    "        \n",
    "    def fit_and_report(self, X, Y, cross_val = True, n_folds=5):\n",
    "        \n",
    "        X = self.vec.fit_transform(X)\n",
    "    \n",
    "        if cross_val:\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "\n",
    "            scores = cross_val_score(self.model, X, Y, cv=n_folds)\n",
    "            print(f\"{n_folds}-fold cross-validation results over training set:\\n\")\n",
    "            print(\"Fold\\tScore\".expandtabs(15))\n",
    "            for i in range(n_folds):\n",
    "                print(f\"{i+1}\\t{scores[i]}\".expandtabs(15))\n",
    "            print(f\"Average\\t{np.mean(scores)}\".expandtabs(15))\n",
    "    \n",
    "        self.model.fit(X, Y)\n",
    "        \n",
    "    def save_model(self, output_file):\n",
    "        with open(output_file, \"wb\") as outfile:\n",
    "            pickle.dump(self, outfile)\n",
    "            \n",
    "    def tag_sentence(self, sentence):\n",
    "        doc = English().tokenizer(sentence)jhgfzuyxzyucx\n",
    "        tokenized_sent = [token.text for token in doc]\n",
    "        featurized_sent = [token_to_features(tokenized_sent, i) \\\n",
    "                           for i in range(len(tokenized_sent))]\n",
    "        vectorized_sent = self.vec.transform(featurized_sent)\n",
    "        labels = self.model.predict(vectorized_sent)\n",
    "        \n",
    "        tagged_sent = list(zip(tokenized_sent, labels))\n",
    "        \n",
    "        return tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I              PRON\n",
      "walked         VERB\n",
      "to             ADP\n",
      "the            DET\n",
      "store          NOUN\n",
      ".              PUNCT\n"
     ]
    }
   ],
   "source": [
    "tagged_sent = tagger.tag_sentence(\"I walked to the store.\")\n",
    "for token in tagged_sent:\n",
    "    print(f\"{token[0]}\\t{token[1]}\".expandtabs(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be enough to get us started. The full model module can be found in `tagger/model.py`. Here is the full code (will not work in Notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from .pre_process import token_to_features\n",
    "\n",
    "class POSTagger:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vec = DictVectorizer()\n",
    "        self.model = LinearSVC()\n",
    "        \n",
    "    def fit_and_report(self, X, Y, cross_val = True, n_folds=5):\n",
    "        \n",
    "        X = self.vec.fit_transform(X)\n",
    "    \n",
    "        if cross_val:\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "\n",
    "            scores = cross_val_score(self.model, X, Y, cv=n_folds)\n",
    "            print(f\"{n_folds}-fold cross-validation results over training set:\\n\")\n",
    "            print(\"Fold\\tScore\".expandtabs(15))\n",
    "            for i in range(n_folds):\n",
    "                print(f\"{i+1}\\t{scores[i]}\".expandtabs(15))\n",
    "            print(f\"Average\\t{np.mean(scores)}\".expandtabs(15))\n",
    "    \n",
    "        self.model.fit(X, Y)\n",
    "        \n",
    "    def save_model(self, output_file):\n",
    "        with open(output_file, \"wb\") as outfile:\n",
    "            pickle.dump(self, outfile)\n",
    "            \n",
    "    def tag_sentence(self, sentence):\n",
    "        doc = English().tokenizer(sentence)\n",
    "        #doc = tokenizer(sentence)\n",
    "        tokenized_sent = [token.text for token in doc]\n",
    "        featurized_sent = [token_to_features(tokenized_sent, i) \\\n",
    "                           for i in range(len(tokenized_sent))]\n",
    "        vectorized_sent = self.vec.transform(featurized_sent)\n",
    "        labels = self.model.predict(vectorized_sent)\n",
    "        \n",
    "        tagged_sent = list(zip(tokenized_sent, labels))\n",
    "        \n",
    "        return tagged_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've written the base code for the `tagger` module, we can write a `run.py` script that will put all the components together and parse the user-input arguments. Before doing so, let's think about the functionality we'd like to enable. Basically, we can see two options so far: training a model and tagging a short sentence using the model. We can incorporate this in an argument called `--mode` which expects the user to call either `train` or `tag`. If a user provides a string that does not correspond to either of these modes, the script will complain and exit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "PARSER = argparse.ArgumentParser(description=\n",
    "                                 \"\"\"\n",
    "                                 A basic SVM-based POS-tagger.\n",
    "                                 Accepts either .conllu or tab-delineated\n",
    "                                 .txt files for training.\n",
    "                                 \"\"\")\n",
    "\n",
    "PARSER.add_argument('--mode', metavar='M', type=str, help=\n",
    "                    \"\"\"\n",
    "                    Specifies the tagger mode: {train, tag}.\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to also add a conditional argument `--text` that should be called whenever `--mode tag` is specified. This will accept a string that will be passed to our `tagger.tag_sentence()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSER.add_argument('--text', metavar='T', type=str, help=\n",
    "                    \"\"\"\n",
    "                    Tags a sentence string.\n",
    "                    Can only be called if '--mode tag' is specified.\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about other parameters at play here. First, if we're calling `--train`, then we need to point our script to a file that can be used for training. Also, we need to tell the script whether or not we'd like to output our cross-validation scores, and, if so, for how many folds. If our training set is large, then this could be an important decision if we're interested in saving time. Lastly, we need to specify where to save our model after training, and where to load it from if we're tagging. \n",
    "\n",
    "Expanding the list of arguments to `ArgumentParser` for all these options could be messy when calling `run.py`. This is especially true if we'd like to modify our code later. A cleaner option could be to include a configuration file, where these remaining parameters are specified. We can do so using `yaml`, which is a human-readable markup language designed for easy serialization. Let's make a `config.yaml` file for our tagger, specifying these remaining parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# tagger config\n",
    "train_file: \"./en_ewt-ud-train.conllu\"\n",
    "model_file: \"./svm_tagger.pickle\"\n",
    "crossval: True\n",
    "n_folds: 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to train a tagger using any other option, we simply have to change this configuration file, or provide a new one. Let's add a final argument to `run.py` for specifying configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSER.add_argument('--config', metavar='C', type=str, help=\n",
    "                    \"\"\"\n",
    "                    A config .yaml file that specifies the train data,\n",
    "                    model output file, and number of folds for cross-validation.\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we're done with specifying arguments for our script (for now). Now let's write a `run()` function that takes our arguments as input and interprets them for the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    mode = args.mode\n",
    "    with open(args.config, \"r\") as yaml_in:\n",
    "        config = yaml.load(yaml_in)\n",
    "    if mode == \"train\":\n",
    "        print(f\"Training a model on {config['train_file']}.\")\n",
    "        X, Y = pre_process.load_dataset(config[\"train_file\"])\n",
    "        X, Y = pre_process.prepare_data_for_training(X, Y)\n",
    "        tagger = model.POSTagger()\n",
    "        tagger.fit_and_report(X, Y, config[\"crossval\"], config[\"n_folds\"])\n",
    "        tagger.save_model(config[\"model_file\"])\n",
    "    elif mode == \"tag\":\n",
    "        print(f\"Tagging text using pretrained model: {config['model_file']}.\")\n",
    "        with open(config[\"model_file\"], \"rb\") as model_in:\n",
    "            tagger = pickle.load(model_in)\n",
    "        tagged_sent = tagger.tag_sentence(args.text)\n",
    "        for token in tagged_sent:\n",
    "            print(f\"{token[0]}\\t{token[1]}\".expandtabs(15))\n",
    "    else:\n",
    "        print(f\"{args.mode} is an incompatible mode. Must be either 'train' or 'tag'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a `run.py` file that can interpret the arguments provided by the user and decide how to deploy the modules we wrote above. Here is the full file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import yaml\n",
    "\n",
    "from tagger import pre_process, model\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    mode = args.mode\n",
    "    with open(args.config, \"r\") as yaml_in:\n",
    "        config = yaml.load(yaml_in)\n",
    "    if mode == \"train\":\n",
    "        print(f\"Training a model on {config['train_file']}.\")\n",
    "        X, Y = pre_process.load_dataset(config[\"train_file\"])\n",
    "        X, Y = pre_process.prepare_data_for_training(X, Y)\n",
    "        tagger = model.POSTagger()\n",
    "        tagger.fit_and_report(X, Y, config[\"crossval\"], config[\"n_folds\"])\n",
    "        tagger.save_model(config[\"model_file\"])\n",
    "    elif mode == \"tag\":\n",
    "        print(f\"Tagging text using pretrained model: {config['model_file']}.\")\n",
    "        with open(config[\"model_file\"], \"rb\") as model_in:\n",
    "            tagger = pickle.load(model_in)\n",
    "        tagged_sent = tagger.tag_sentence(args.text)\n",
    "        for token in tagged_sent:\n",
    "            print(f\"{token[0]}\\t{token[1]}\".expandtabs(15))\n",
    "    else:\n",
    "        print(f\"{args.mode} is an incompatible mode. Must be either 'train' or 'tag'.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PARSER = argparse.ArgumentParser(description=\"A basic SVM-based POS-tagger. \\\n",
    "                                     Accepts either .conllu or tab-delineated \\\n",
    "                                     .txt files for training.\")\n",
    "\n",
    "    PARSER.add_argument('--mode', metavar='M', type=str,\n",
    "                        help=\"Specifies the tagger mode: {train, tag, eval}.\")\n",
    "    PARSER.add_argument('--text', metavar='T', type=str,\n",
    "                        help=\"Tags a sentence string. \\\n",
    "                        Can only be called if '--mode tag' is specified.\")\n",
    "    PARSER.add_argument('--config', metavar='C', type=str,\n",
    "                        help=\"A config .yaml file that specifies the train data, \\\n",
    "                        model output file, and number of folds for cross-validation.\")\n",
    "\n",
    "    ARGS = PARSER.parse_args()\n",
    "\n",
    "    run(ARGS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just written a package for part-of-speech tagging in Python! Using the principles introduced here, you should now be well-equipped to write your own machine learning code, on top of `scikit-learn` or any other library. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
