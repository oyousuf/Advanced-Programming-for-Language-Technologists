{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Programming for Language Technologists\n",
    "Artur Kulmizev, Spring 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites:\n",
    "\n",
    "For this lecture, we'll be working with several Python libraries. A good way to keep track of the libraries you may use for various projects is to use virtual environments. Anaconda is a data science-oriented Python distribution that allows for easy management of virtual environments. This is done via the `conda` command line interface. You can download the base version Anaconda for your platform [here](https://www.anaconda.com/products/individual), or Miniconda [here](https://docs.conda.io/en/latest/miniconda.html). While the base Anaconda comes with many relevant packages pre-installed and is useful for this class, Miniconda might be a good choice if you are working with limited disk space and simply need access to the `conda` functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the Anaconda (or Miniconda) installation, you can create a virtual environment as follows:\n",
    "\n",
    "`$ conda create -n web_scraping python==3.7`\n",
    "\n",
    "After doing this, an environment named `web_scraping` will be created in your local installation of Anaconda, with Python 3.7 as the base version. This will hold all the necessary packages that you'll be working with on this project. \n",
    "\n",
    "Note that this simply creates the environment. In order to activate it, run:\n",
    "\n",
    "`$ conda activate web_scraping`\n",
    "\n",
    "After loading an env, you should see its name at the beginning of your shell prompt:\n",
    "\n",
    "`(web_scraping)$`\n",
    "\n",
    "Once you're inside an environment, you can install relevant packages as follows:\n",
    "\n",
    "`(web_scraping)$ pip install beautifulsoup4 selenium`\n",
    "\n",
    "After the packages have completed installing, their installation will exist within this folder, isolated from the rest of the system. After deactivating the environment, your Python settings will revert to how they were before. To deactivate an environment, you can simply run `(web_scraping)$ deactivate`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1: Web Scraping\n",
    "\n",
    "### _What is web scraping?_\n",
    "\n",
    "Web scraping refers to the practice of writing computer programs that can extract data from web pages. People typically scrape the web for images or text, but other types of data - like geolocation and demographics - are also very prevalent. \n",
    "\n",
    "### _How is web scraping relevant to LT?_\n",
    "\n",
    "As language technologists-in-training, so far you've mostly worked with curated corpus text. This type of text (whether annotated or not) is immensely useful for training machine learning-based NLP systems, but is often very limited in terms of access and scope. \n",
    "\n",
    "Say you wanted to use Twitter data to investigate dialect differences across various regions of Switzerland (there are a lot). You'll likely need a dataset or corpus of Tweets, along with user-provided geolocation or something of the sort. Perhaps there is an existing dataset you could use, but, in most cases, there likely isn't. Similarly, what if you did manage to get your hands on a perfect corpus, but it only collected Tweets from 2010? Given the rapid pace of language change on the internet, it is unlikely that the data would be sufficiently up-to-date for your purposes. In such cases, you'll probably need to get the data yourself. \n",
    "\n",
    "### _How can we do it?_\n",
    "\n",
    "Typically, when you've determined that you need to acquire your own data from the internet, there are two scenarios that arise. \n",
    "\n",
    "#### Easy Scenario: the website provides an API:\n",
    "\n",
    "API stands for Application Programming Interface. APIs are essentially enterprise-provided tools and guidelines that help users programatically interact with their services. Instead of attempting to navigate the maze of data structures in a particular website from scratch, users can use a website's API to make the process significantly easier - most of the work has already been done for them. For example, [Twitter offers an API](https://developer.twitter.com/en/docs) that specifies how one can retrieve Tweets from a specific time/date, post one's own Tweets, and find relevant information, like geolocation, etc. This can then be used to write programs in the user's language of choice, say, Python. \n",
    "\n",
    "Unfortunately, APIs are often only provided by enterprise systems like Twitter, Google, and Facebook. As such, someone looking to crawl disparate sources of information (e.g. English-language blog posts about the Iraq War circa 2001-2004) will need to get creative.\n",
    "\n",
    "#### Hard Scenario: the website provides no API:\n",
    "\n",
    "In the case that you don't have access to a website's API, you'll probably have to interact with webpages directly. Webpages are built upon HTML, which stands for Hypertext Markup Language. Scraping is typically done by processing lots of pages, parsing their HTML structure, and returning relevant bits of that structure, like paragraphs of text. Though this might seem daunting, there are a multitude of libraries that can help you make this process relatively painless. For the sake of this tutorial, we'll focus on these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML review\n",
    "\n",
    "A very simple HTML page might look something like this.\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>My Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "    <h1>My First Heading</h1>\n",
    "    <p>My first paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html> \n",
    "```\n",
    "\n",
    "HTML documents (just webpages) consist of elements that tell browsers how to display content. Elements are typically things like headings, paragraphs, lists, that can be used to format, structure, and stylize webpages. (Most) elements have appear between between tags, which specify their beginning and ends, like so: `<h1>This is a heading</h1>`. A typical HTML document consists of the following structure:\n",
    "\n",
    "* a `<!DOCTYPE html>` declaration, which specifies to the browser that the document is indeed an HTML5 document.\n",
    "* an `<html>` element that contains the entire page's contents.\n",
    "* a `<head>` element that contains information and metadata about the page, like its title, etc.\n",
    "* a `<body>` element that contains the main part of the page: its structure, headings, text, etc.\n",
    "\n",
    "The goal of web scraping is to extract information that is structured in this format. It is important to note that very, very few webpages have simple HTML-parsable formats. Also, HTML isn't the only language at play in web development - far from it. A typical webpage might consist of a complicated interplay of various languages: HTML, JavaScript, Ruby, etc. This is what makes scraping a challenging task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we parse HTML with Python?\n",
    "\n",
    "Naively? Regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "my_page = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>My Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "    <h1>My First Heading</h1>\n",
    "    <p>My first paragraph.</p>\n",
    "\n",
    "    <h1>My Second Heading</h1>\n",
    "    <p>My second paragraph.</p>\n",
    "    \n",
    "</body>\n",
    "</html> \n",
    "'''\n",
    "\n",
    "#Search for the title of the page\n",
    "\n",
    "title_search = re.compile(r'<h1.*?>(.+?)</h1>')\n",
    "title = title_search.findall(my_page)\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for very simple, toy HTML pages. With more complex HTML pages, the code becomes messy and unpredictable, so regex is a waste of time. \n",
    "\n",
    "Better to use an HTML parser, like Beautiful Soup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# We can call BeautifulSoup on our page to parse the document\n",
    "\n",
    "soup = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "# This gives us a parsed object that we can interact with\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract any element out of the document...\n",
    "\n",
    "print(soup.title)\n",
    "\n",
    "# ... as well as its corresponding text\n",
    "\n",
    "print(soup.title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use soup to find the first instance of an element...\n",
    "\n",
    "print(soup.p)\n",
    "\n",
    "# ... or all instances of an element...\n",
    "\n",
    "print(soup.find_all(\"p\"))\n",
    "\n",
    "# ... and their corresponding text\n",
    "\n",
    "for paragraph in soup.find_all(\"p\"):\n",
    "    print(paragraph.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to parse live web pages? We can do the following using `urllib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "site_string = \"https://cl.lingfil.uu.se/~nivre/\"\n",
    "\n",
    "jn_site = urllib.request.urlopen(site_string)\n",
    "soup2 = BeautifulSoup(jn_site, \"html.parser\")\n",
    "\n",
    "print(soup2.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptions\n",
    "\n",
    "Let's try loading the webpages of several computational linguists at Uppsala and see what they call their page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"gongbo\", \"nivre\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    cl_site = urllib.request.urlopen(site_string)\n",
    "    soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "    print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our code ran for the first three names, but failed on mine. The last site caused `urrlib` to raise an `HTTPError`, which was raised here: `--> 649         raise HTTPError(req.full_url, code, msg, hdrs, fp)`. Let's look at some other types of exceptions in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeroDivisionError\n",
    "\n",
    "12/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NameError \n",
    "\n",
    "variable_we_did_not_define + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypeError\n",
    "\n",
    "12 + \"cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can handle exceptions using `try` and `except` statements. `try` tells Python to try running your code, with the expectation that something might go awry. `except` tells Python that, when something does go awry, you have code to handle the situation accordingly. Let's try them in our code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"gongbo\", \"nivre\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    try:\n",
    "        cl_site = urllib.request.urlopen(site_string)\n",
    "        soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "        print(soup.title.string)\n",
    "    except:\n",
    "        print(\"Site not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more specificity, we can include the exact type of exception we're hoping to catch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "\n",
    "for name in [\"gongbo\", \"nivre\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    try:\n",
    "        cl_site = urllib.request.urlopen(site_string)\n",
    "        soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "        print(soup.title.string)\n",
    "    except HTTPError:\n",
    "        print(\"Site not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `else` statements to have more control over the program. Like in `if-else` statments, `try-else` works as a fallback to when an exception is *not* caught. For the purposes of this code, then, we can just move `print(soup.title.string)` inside `else`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"gongbo\", \"nivre\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    try:\n",
    "        cl_site = urllib.request.urlopen(site_string)\n",
    "        soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "    except HTTPError:\n",
    "        print(\"Site not found.\")\n",
    "    else:\n",
    "        print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use `finally` statements to execute code regardless of the status of the exception checker. This code will always execute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"gongbo\", \"nivre\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    try:\n",
    "        cl_site = urllib.request.urlopen(site_string)\n",
    "        soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "    except HTTPError:\n",
    "        print(\"*SITE NOT FOUND.*\")\n",
    "    else:\n",
    "        print(soup.title.string)\n",
    "    finally:\n",
    "        print(f\"Finished processing page for {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more challenging example: scraping presidential speeches\n",
    "\n",
    "So far, we've been working with fairly straightforward HTML pages that are easy to scrape. In most situations, this won't be the case. For the next module, we will be working with the text transcripts of all US presidential speeches. However, there is no corpus that collects this information. Luckily, The Miller Center (a University of Virginia research center that studies the history of the US presidency) has made all popular presidential speeches public at [this address](https://millercenter.org/the-presidency/presidential-speeches). We can use these, but since the raw text of these speeches is not available for download, we will have to scrape the necessary content ourselves. \n",
    "\n",
    "Judging from the site, it appears that actual transcripts live on separate pages, which are structured like this: https://millercenter.org/the-presidency/presidential-speeches/january-8-2020-statement-iran. It is these pages that we will need to parse using BeautifulSoup. However, in order to do so, we need to know the links to each individual page. Looking back at the main page, it seems that the links to the transcript pages appear inside a `div` element class called `views-row`. If we look inside these `views-row` blocks, we should be able to extract the anchor `href` elements that point to the corresponding transcript links. Unfortunately, the structure of the page is set in such a way that only 12 `views-row` blocks are loaded at a time. The way to load a new set of blocks is to scroll down the page. This mechanism is called an _infinite scroll_ and is notoriously difficult to scrape. What we need to do in this case is to _keep scrolling down_ until we see the full set of links on the page. This is a tedious task to do manually, so we can use a package called `selenium`, which allows you to write programs that take control of your browser. Using `selenium`, we will open the site and continually press the Page Down key (<kbd>COMMAND</kbd>+<kbd>DOWN</kbd> on Mac OSX) until we've loaded the full set of transcript links. Once the full page is loaded, we can extract the `href` elements from all `views-row` blocks and create a list of links, which we will later individually parse using BeautifulSoup. \n",
    "\n",
    "Let's look at how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Tell selenium that we will open the page using Safari.\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "# Open the page in Safari.\n",
    "browser.get('https://millercenter.org/the-presidency/presidential-speeches')\n",
    "\n",
    "# Find the element that we need to interact with, in this case the 'body'.\n",
    "elem = browser.find_element_by_tag_name('body')\n",
    "\n",
    "# Specify the number of times we need to scroll down. 150 should be OK.\n",
    "no_of_pagedowns = 200\n",
    "\n",
    "# While we still need to scroll...\n",
    "while no_of_pagedowns: \n",
    "    \n",
    "    # Press COMMAND+DOWN to scroll down.\n",
    "    elem.send_keys(Keys.COMMAND+Keys.DOWN)\n",
    "    \n",
    "    # Wait a second so that the new set of links can load.\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "    # De-increment the number of page-downs.\n",
    "    no_of_pagedowns-=1\n",
    "    \n",
    "end = time.time()\n",
    "print(f\"Process took {end - start} seconds...\")\n",
    "\n",
    "# Now that the entire page is loaded, extract all links in the block class \n",
    "post_links = browser.find_elements_by_css_selector(\".views-row a\")\n",
    "\n",
    "# Initialize an empty list to populate with links.\n",
    "links = []\n",
    "\n",
    "# For all anchors we found...\n",
    "for link in post_links: \n",
    "    \n",
    "    # Extract the 'href' element...\n",
    "    href = link.get_attribute('href')\n",
    "    \n",
    "    # ... and append its contents to the links list. \n",
    "    links.append(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the full list of links, we should probably save it so that we don't have to re-run the extraction script. We can do so using JSON, which is a syntax for storing data structures like lists, dictionaries, etc. This storing process is called *serialization*. When we serialize an object, we _encode_ it into a series of bytes that can later be _decoded_ or _deserialized_. Let's try to do this for our list in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"speech_links.json\", \"w\") as outfile:\n",
    "    json.dump(links, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our list is saved in the file named `speech_links.json`. If we'd like to de-serialize a JSON file and load it into a Python object, we can do so in a similar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"speech_links.json\", \"r\") as infile:\n",
    "    links = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code opens the JSON file, deserializes it, and loads it into a Python list object, which we can interact with. \n",
    "\n",
    "Now let's define a function that can take a link, parse its HTML and extract a cleaned version of the transcript text. We can use a dictionary to make it easy to search for presidents by their names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import unicodedata library to help with cleaning scraped text\n",
    "import unicodedata\n",
    "\n",
    "# Define a function that takes a miller center speech transcript link as an argument\n",
    "def get_speeches(link):\n",
    "    \n",
    "    # Get HTML from live web page\n",
    "    html_body = urllib.request.urlopen(link)\n",
    "    \n",
    "    # Parse HTML with BS\n",
    "    soup = BeautifulSoup(html_body,'html.parser')\n",
    "    \n",
    "    # Select appropriate element for transcript\n",
    "    transcript = soup.select('.transcript-inner p')\n",
    "    \n",
    "    # If the speech is captured on video, we need to instead click on \"View Transcript\"\n",
    "    if len(transcript)<=0:\n",
    "        transcript = soup.select('.view-transcript p')\n",
    "        \n",
    "    # Go through every <p> that BS finds, extract the text, and clean it\n",
    "    transcripts = [unicodedata.normalize(\"NFKD\", elem.get_text()) for elem in transcript]\n",
    "    \n",
    "    # Go through list of <p>'s and join them into a single string\n",
    "    speech = ' '.join(transcripts)\n",
    "    \n",
    "    # Get the speechgiver's name\n",
    "    president_name = soup.select('.president-name')[0].get_text()\n",
    "    speech_date = soup.select(\".episode-date\")[0].get_text()\n",
    "    # Create a dictionary consisting of two items: \n",
    "    # * the president's name\n",
    "    # * the president's speech\n",
    "    speech_dic = {'Name': president_name, 'Speech': speech, 'Date': speech_date}\n",
    "    \n",
    "    return speech_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_speeches(links[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's loop through our list of links and extract the speeches. We will save these dictionaries as single `.json` files in a directory called `us_presidential_speeches`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "speech_dir = \"../files/us_presidential_speeches_test\"\n",
    "\n",
    "os.mkdir(speech_dir)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Iterate through links in reverse order...\n",
    "for i, link in enumerate(links[::-1]):\n",
    "    if i > 0 and i % 25 == 0:\n",
    "        current = time.time()\n",
    "        print(f\"Formatted {i} speeches. Took {current - start} so far...\")\n",
    "    speech_idx = str(i)\n",
    "    if len(speech_idx) == 1:\n",
    "        speech_idx = f\"00{speech_idx}\"\n",
    "    elif len(speech_idx) == 2:\n",
    "        speech_idx = f\"0{speech_idx}\"\n",
    "    else:\n",
    "        pass\n",
    "    with open(speech_dir+f\"/speech_{speech_idx}.json\", \"w\") as speech_out:\n",
    "        speech_dic = get_speeches(link)\n",
    "        json.dump(speech_dic, speech_out)\n",
    "        \n",
    "end = time.time()\n",
    "print(f\"Finished. Took {end - start} in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll leave off here. Next lecture, we'll see how we can work with this raw text, transform it into a numeric format, and visualize how different speeches may relate to each other quantitatively. \n",
    "\n",
    "Web scraping code largely adapted (stolen) from [here](https://github.com/hajir-almahdi/the-data-behind-presidental-charisma/blob/master/The-Data-Behind-Presidental-Charisma.ipynb). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
